{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecondStage2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJlG_lwKEaQv",
        "colab_type": "text"
      },
      "source": [
        "to do:\n",
        "- image in tensorboard (test alpha mobilenet)\n",
        "- imports aufräumen\n",
        "- Secondstage.py\n",
        "- Konfiguration außerhalb:\n",
        "    - Name output\n",
        "    - cat/ bin/ reg\n",
        "- endliche Anzahl an Runden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHIKxNY3oahf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VO6zc_v7lJ-",
        "colab_type": "text"
      },
      "source": [
        "object_detection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_HjEoWRonVL",
        "colab_type": "code",
        "outputId": "4b11bf34-1361-4c23-8805-29ce9c695b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH4BMdyyorVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85f2szA_pAJg",
        "colab_type": "code",
        "outputId": "e379bb9a-09dd-4fc6-cc96-d137e24c0596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd models/research"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'models/research'\n",
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPwg_kkapFJg",
        "colab_type": "code",
        "outputId": "04e756ed-e07d-4f24-e6c3-81e1f650e997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "object_detection/protos/*.proto: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsskp5qWpJRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJLXPuYIpNg4",
        "colab_type": "code",
        "outputId": "6487e63d-06b8-44b1-cf1a-926efe307cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'object_detection/builders/model_builder_test.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "japGTz-bFnCC",
        "colab_type": "code",
        "outputId": "1254fe70-793d-4ab3-e065-5a962cfd1cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5h9RbyaYC-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/SecondStage_colab/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZWtkiVKShNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['PYTHONPATH'] += ':/content/gdrive/My Drive/SecondStage_colab/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exoPdhiJ7p1R",
        "colab_type": "text"
      },
      "source": [
        "Second Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwRKHys0KBHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NAME =\n",
        "#example: cat_exptype_dataset(training)_dataset(eval)_conf(batch size etc) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqzYclgV64T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_PATH = '/content/gdrive/My Drive/SecondStage_colab/output/'\n",
        "LABEL_MAP_PATH = '/content/gdrive/My Drive/SecondStage_colab/label_map.pbtxt'\n",
        "TRAIN_RECORD = '/content/gdrive/My Drive/data/generated/SecondStage_X/training_rot3.record'\n",
        "EVAL_RECORD = '/content/gdrive/My Drive/data/generated/SecondStage_X/validation_rot2.record'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSl3lC_tl3oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRAIN_RECORD = 'training_rot15.record'\n",
        "#EVAL_RECORD = 'validation_rot2.record'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjrsBWe_7sjQ",
        "colab_type": "code",
        "outputId": "166d0880-9439-45f5-b6d3-27fcaede5e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/')\n",
        "!ls\n",
        "from SecondStage_colab import second_stage_utils\n",
        "from SecondStage_colab import file_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'8-Week General Cycle.md'\n",
            "'Anmeldung Bierpongturnier (Antworten).gsheet'\n",
            "'Anmeldung Bierpongturnier.gform'\n",
            " Assignment3\n",
            "'BA - Rolling Swarm.gdoc'\n",
            "'Colab Notebooks'\n",
            " data\n",
            "'Deadlift Program.gsheet'\n",
            " DHBW_App_Projektstruktur_rev1.02.pdf\n",
            " events.out.tfevents.1579157229.5ae34ead26e8\n",
            " events.out.tfevents.1579157352.5ae34ead26e8\n",
            " events.out.tfevents.1579184572.b50e20e5cc32\n",
            " Ferienhäuser.gsheet\n",
            "'HSPU Progression.gdoc'\n",
            "'HSPU Progression.gsheet'\n",
            " Klim.gdoc\n",
            "'Kopie von Links 1. Semester (1).gsheet'\n",
            "'Kopie von Links 1. Semester.gsheet'\n",
            "'Kündigung CFRN.gdoc'\n",
            "'Kündigung EVG.gdoc'\n",
            "'Kündigung Fitness Schleusingen.gdoc'\n",
            "'Kündigung - Zeller.gdoc'\n",
            " Laptops.gsheet\n",
            "'Links 1. Semester.gsheet'\n",
            " object_detection\n",
            "'Ohne Titel.gdoc'\n",
            "'Pflichtenheft – Team EEG.gdoc'\n",
            "'Projektablauf und -strutkurplan.gsheet'\n",
            " Risiko.gsheet\n",
            " Second_Stage\n",
            " SecondStage_colab\n",
            "'TechInf 1.gdoc'\n",
            " TechInf1.gdoc\n",
            "'TechInf 2.gdoc'\n",
            "'TechInf 3.gdoc'\n",
            "'TechInf 5.gdoc'\n",
            "'TI2 - Blatt 06.gdoc'\n",
            "'TI2 - Blatt2.gdoc'\n",
            "'TI2 - Blatt 4.gdoc'\n",
            "'TI - Blatt 07.gdoc'\n",
            "'TI - Blatt 09.gdoc'\n",
            "'TI Blatt 11.gdoc'\n",
            " To-do-Liste.gsheet\n",
            " Training.gsheet\n",
            "'Unbenannte Präsentation.gslides'\n",
            "'Unbenanntes Dokument (1).gdoc'\n",
            "'Unbenanntes Dokument (2).gdoc'\n",
            "'Unbenanntes Dokument (3).gdoc'\n",
            "'Unbenanntes Dokument (4).gdoc'\n",
            "'Unbenanntes Dokument (5).gdoc'\n",
            "'Unbenanntes Dokument (6).gdoc'\n",
            "'Unbenanntes Dokument (7).gdoc'\n",
            "'Unbenanntes Dokument.gdoc'\n",
            "'Unbenannte Tabelle (1).gsheet'\n",
            "'Unbenannte Tabelle (2).gsheet'\n",
            "'Unbenannte Tabelle (3).gsheet'\n",
            "'Unbenannte Tabelle (4).gsheet'\n",
            "'Unbenannte Tabelle.gsheet'\n",
            " Untitled.md\n",
            "'Votierungen Mathe.gsheet'\n",
            "'Votierungen WS18 19.gsheet'\n",
            "'WG Anzeige.gdoc'\n",
            " WG.gdoc\n",
            "'WG Suche - Anfragen'\n",
            "'WG Suche .gdoc'\n",
            "'WG Suche - Rückmeldungen.gsheet'\n",
            "'WhatsApp Chat - Chris WG.zip'\n",
            " Zeitplan.gdoc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKaABMtXB-OA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/models/research')\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R7pUpyVcHIf",
        "colab_type": "text"
      },
      "source": [
        "configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1i_AkgfcGUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = True\n",
        "BATCH_SIZE = 7425\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Hbp0ghcZtB",
        "colab_type": "text"
      },
      "source": [
        "label map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk0ZiaTCthBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/SecondStage_colab')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOwjdrteceEe",
        "colab_type": "code",
        "outputId": "89701e97-68c3-4179-e492-16d736548d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'red'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'orange'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'yellow'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 4\n",
        "  name: 'lime_green'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 5\n",
        "  name: 'magenta' \n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 6\n",
        "  name: 'purple'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 7\n",
        "  name: 'green'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 8\n",
        "  name: 'light_green'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 9\n",
        "  name: 'blue_green'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 10\n",
        "  name: 'light_blue'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 11\n",
        "  name: 'blue'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5-N8oYNQ_8C",
        "colab_type": "text"
      },
      "source": [
        "experiment definitioin\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmm4SaY3QXt3",
        "colab_type": "code",
        "outputId": "501c2705-c1a2-4612-8477-754f45dd54b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile exp_def.py\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "default_sstage_conf = {\n",
        "    'dataset': 'default',\n",
        "    'types': ['copter','sphero','youbot'],\n",
        "    # converged after 15 / 10 / 5 epochs\n",
        "    # 10 epochs after convergance for evaluation phase\n",
        "    'epochs_cat': 60,\n",
        "    #'epochs_reg': 20,\n",
        "    'epochs_reg': 60,\n",
        "    'epochs_bin': 60,\n",
        "    'optimizer': 'adam',\n",
        "    'learning_rate': 3e-4,\n",
        "    'dropout': 0,\n",
        "    'alpha': 0.5,\n",
        "    'img_size': 35,\n",
        "    'separate_cat_ori': True,\n",
        "    'cat_weight': 1.0,\n",
        "    'reg_weight': 1.0,\n",
        "    'bin_weight': 1.0,\n",
        "    'enable_reg': False,\n",
        "    'enable_bin': True,\n",
        "    'repetions': 4,\n",
        "}\n",
        "\n",
        "def create_all_sstage_experiments():\n",
        "    configs = []\n",
        "    configs.extend(create_sstage_default())\n",
        "    configs.extend(bin_mod())\n",
        "    return configs\n",
        "\n",
        "def create_sstage_default():\n",
        "    config = []\n",
        "    modified = deepcopy(default_sstage_conf)\n",
        "    modified['name'] = \"sstage_default\"\n",
        "    modified['enable_reg'] = True\n",
        "    config.append(deepcopy(modified))\n",
        "    return config\n",
        "\n",
        "def bin_mod():\n",
        "    config = []\n",
        "    modified = deepcopy(default_sstage_conf)\n",
        "    modified['name'] = \"bin_mod\"\n",
        "    modified['enable_reg'] = True\n",
        "    config.append(deepcopy(modified))\n",
        "    return config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting exp_def.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDJatt09QfGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "from datetime import datetime\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from keras import applications\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Reshape, Conv2D, Activation\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, Callback\n",
        "from keras.models import load_model\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "import csv, json, pickle\n",
        "from lxml import etree\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG9YYdriX8WZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_record, conf, types, out, rep=1):\n",
        "\n",
        "    from file_utils import save_json\n",
        "    from second_stage_utils import tf_record_load_crops, tf_record_extract_crops, data_to_keras, angle_to_bin, angle_mse, angle_mae, angle_bin_error, TensorBoardCustom\n",
        "\n",
        "    timestamp = \"{:%Y-%m-%d-%H-%M}\".format(datetime.now())\n",
        "    log_path = LOG_PATH + conf['name'] + '_' + \\\n",
        "                ''.join(types) + out + '/' + \\\n",
        "                timestamp + '-r' + str(rep) + '/'\n",
        "    os.makedirs(log_path, exist_ok=True)\n",
        "    save_json(log_path + '/experiment_config.json', conf)\n",
        "\n",
        "    label_map = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
        "    #label_map = create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
        "\n",
        "    num_classes = label_map_util.get_max_label_map_index(\n",
        "                           label_map_util.load_labelmap(LABEL_MAP_PATH)) + 1\n",
        "    print(\"x\")\n",
        "\n",
        "    X,Y,Z,_ = tf_record_load_crops([train_record])\n",
        "\n",
        "    \"\"\"\n",
        "    for i in range(ROT_SIZE):\n",
        "      X.extend(X)\n",
        "      Y.extend(Y)\n",
        "      Z.extend(Z)\n",
        "      _.extend(_)\n",
        "    \"\"\"\n",
        "    \n",
        "    X_train, Y_train, Z_train = data_to_keras(X,Y,Z,num_classes,conf['img_size'])\n",
        "    Z2_train = angle_to_bin(Z_train)\n",
        "    print(\"´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\")\n",
        "    print(Y_train.shape)\n",
        "    print(num_classes)\n",
        "    print(Z2_train.shape)\n",
        "    print(NUM_ORI_BINS)\n",
        "    print(\"´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\")\n",
        "\n",
        "    eval_records = EVAL_RECORD\n",
        "\n",
        "    X,Y,Z,_ = tf_record_extract_crops([eval_records], 1, 0.0, 0.0)\n",
        "    X_val, Y_val, Z_val = data_to_keras(X,Y,Z,num_classes,conf['img_size'])\n",
        "    Z2_val = angle_to_bin(Z_val)\n",
        "\n",
        "    assert len(X_val) > 0 and len(Y_val) > 0 and len(Z_val) > 0, \\\n",
        "        '{} is incomplete'.format(eval_records)\n",
        "    mobilenet_base = applications.mobilenet.MobileNet(alpha = conf['alpha'],\n",
        "                                                      weights = \"imagenet\",\n",
        "                                                      include_top=False,\n",
        "                                                      input_shape = (\n",
        "                                                      conf['img_size'],\n",
        "                                                      conf['img_size'],\n",
        "                                                        3\n",
        "                                                      ))\n",
        "    shape = (1, 1, int(1024 * conf['alpha']))\n",
        "    x = GlobalAveragePooling2D()(mobilenet_base.output)\n",
        "    x = Reshape(shape, name='reshape_1')(x)\n",
        "    x = Dropout(conf['dropout'], name='dropout')(x)\n",
        "    # Branch regression\n",
        "    reg = Conv2D(1, (1, 1),\n",
        "               padding='same', name='conv_reg')(x)\n",
        "    reg = Activation('linear', name='act_linear')(reg)\n",
        "    reg = Reshape((1,), name='reg_out')(reg)\n",
        "    # Branch orientation classification with bins\n",
        "    bin = Conv2D(NUM_ORI_BINS, (1, 1),\n",
        "               padding='same', name='conv_bin')(x)\n",
        "    bin = Activation('softmax', name='act_bin')(bin)\n",
        "    bin = Reshape((NUM_ORI_BINS,), name='bin_out')(bin)\n",
        "    # Branch classification\n",
        "    cat = Conv2D(num_classes, (1, 1),\n",
        "               padding='same', name='conv_cat')(x)\n",
        "    cat = Activation('softmax', name='act_softmax')(cat)\n",
        "    cat = Reshape((num_classes,), name='cat_out')(cat)\n",
        "\n",
        "    # creating the final model\n",
        "    model_final = None\n",
        "    model_final = Model(inputs = mobilenet_base.input, outputs = [cat,reg,bin])\n",
        "\n",
        "    if conf['optimizer'] == 'rmsprop':\n",
        "        optimizer = optimizers.RMSprop(lr=conf['learning_rate'])\n",
        "    elif conf['optimizer'] == 'adam':\n",
        "        optimizer = optimizers.Adam(lr=conf['learning_rate'])\n",
        "    else:\n",
        "        raise Error('Unknown optimizer: ' + conf['optimizer'])\n",
        "\n",
        "    model_final.compile(optimizer = optimizer,\n",
        "                        loss={'cat_out': 'categorical_crossentropy',\n",
        "                              'reg_out': angle_mse,\n",
        "                              'bin_out': 'categorical_crossentropy',\n",
        "                              #'bin_out': 'binary_crossentropy'\n",
        "                        },\n",
        "                        loss_weights={'cat_out': conf['cat_weight'],\n",
        "                                      'reg_out': conf['reg_weight'],\n",
        "                                      'bin_out': conf['bin_weight']},\n",
        "                        metrics ={'cat_out': 'accuracy',\n",
        "                                  'reg_out': angle_mae,\n",
        "                                  'bin_out': angle_bin_error})\n",
        "\n",
        "    summary = TensorBoardCustom(log_dir=log_path,label_map=label_map)\n",
        "    filepath=log_path+\"model-{epoch:02d}.h5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, verbose=1, period=20)\n",
        "\n",
        "    \"\"\"callbacks=[summary,checkpoint],\"\"\"\n",
        "\n",
        "    model_final.fit(\n",
        "        X_train,\n",
        "        {'cat_out': Y_train, 'reg_out': Z_train, 'bin_out': Z2_train},\n",
        "        validation_data=(X_val,[Y_val,Z_val, Z2_val]),\n",
        "        batch_size=BATCH_SIZE, epochs=conf['epochs'], verbose=0,\n",
        "        callbacks=[summary,checkpoint],\n",
        "        shuffle=True\n",
        "    )\n",
        "    model_final.save(log_path+\"model-final.h5\")\n",
        "    print(\"Finished training for {}_{}\".format(conf['name'],t))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjhzN9oBxzHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from SecondStage_colab import exp_def\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jru6gJiHAVl",
        "colab_type": "code",
        "outputId": "5c8f68e9-fab2-45d3-fb3f-a06244974606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ANGLES_PER_BIN = 1\n",
        "NUM_ORI_BINS = 360\n",
        "\n",
        "exp = exp_def.create_all_sstage_experiments()\n",
        "print(BATCH_SIZE)\n",
        "for or_conf in tqdm(exp):\n",
        "    print(\"or_conf\", or_conf)\n",
        "    for r in range(or_conf['repetions']): \n",
        "        #train_records = get_recursive_file_list(TRAIN_DIR,file_matchers=[or_conf['dataset']])\n",
        "        train_records = [TRAIN_RECORD]\n",
        "        for t in or_conf['types']:\n",
        "            for out in ['_cat','_reg','_bin','']:\n",
        "                conf = deepcopy(or_conf)\n",
        "                train_instance_name = conf['name']+'_'+t+out\n",
        "                if not conf['separate_cat_ori'] and out != '':\n",
        "                    continue\n",
        "                if conf['separate_cat_ori']:\n",
        "                    if out == '':\n",
        "                        continue\n",
        "                    elif out == '_cat':\n",
        "                        conf['reg_weight'] = 0.0\n",
        "                        conf['bin_weight'] = 0.0\n",
        "                        conf['epochs'] = conf['epochs_cat']\n",
        "                    elif out == '_reg' and conf['enable_reg']:\n",
        "                        conf['cat_weight'] = 0.0\n",
        "                        conf['bin_weight'] = 0.0\n",
        "                        conf['epochs'] = conf['epochs_reg']\n",
        "                    elif out == '_bin' and conf['enable_bin']:\n",
        "                        conf['cat_weight'] = 0.0\n",
        "                        conf['reg_weight'] = 0.0\n",
        "                        conf['epochs'] = conf['epochs_bin']\n",
        "                    else:\n",
        "                        print('UNKNOWN OUTPUT CONFIG {}'.format(out))\n",
        "                        continue\n",
        "                if not re.match('sstage_default_sphero_cat', train_instance_name): \n",
        "                    print('Skip '+train_instance_name)\n",
        "                    continue            \n",
        "                #record_for_type = [e for e in train_records if t in e]\n",
        "                #print(\"record_for_type: \", len(record_for_type))\n",
        "                #assert len(record_for_type) == 1, \\\n",
        "                #       \"{} for {} has not one item\".format(record_for_type, train_instance_name)\n",
        "                print(\"Start training\")\n",
        "                \n",
        "                if not GPU:\n",
        "                    p = Process(\n",
        "                      target=train,\n",
        "                      args=(train_records[0], conf, ['sphero'], out, r)\n",
        "                    )\n",
        "                    p.start() \n",
        "                else:\n",
        "                 \n",
        "                  train(train_records[0], conf, ['sphero'], out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7425\n",
            "or_conf {'dataset': 'default', 'types': ['copter', 'sphero', 'youbot'], 'epochs_cat': 60, 'epochs_reg': 60, 'epochs_bin': 60, 'optimizer': 'adam', 'learning_rate': 0.0003, 'dropout': 0, 'alpha': 0.5, 'img_size': 35, 'separate_cat_ori': True, 'cat_weight': 1.0, 'reg_weight': 1.0, 'bin_weight': 1.0, 'enable_reg': True, 'enable_bin': True, 'repetions': 4, 'name': 'sstage_default'}\n",
            "Skip sstage_default_copter_cat\n",
            "Skip sstage_default_copter_reg\n",
            "Skip sstage_default_copter_bin\n",
            "Start training\n",
            "x\n",
            "Loaded 59400 crops from ['/content/gdrive/My Drive/data/generated/SecondStage_X/training_rot3.record']\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n",
            "(59400, 12)\n",
            "12\n",
            "(59400, 360)\n",
            "360\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_5_0_224_tf_no_top.h5\n",
            "5578752/5577668 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/SecondStage_colab/second_stage_utils.py:22: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/SecondStage_colab/second_stage_utils.py:181: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/SecondStage_colab/second_stage_utils.py:182: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/SecondStage_colab/second_stage_utils.py:187: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-19-19-10-r1/model-20.h5\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-19-19-10-r1/model-40.h5\n",
            "\n",
            "Epoch 00060: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-19-19-10-r1/model-60.h5\n",
            "Finished training for sstage_default_sphero\n",
            "Skip sstage_default_sphero_reg\n",
            "Skip sstage_default_sphero_bin\n",
            "Skip sstage_default_youbot_cat\n",
            "Skip sstage_default_youbot_reg\n",
            "Skip sstage_default_youbot_bin\n",
            "Skip sstage_default_copter_cat\n",
            "Skip sstage_default_copter_reg\n",
            "Skip sstage_default_copter_bin\n",
            "Start training\n",
            "x\n",
            "Loaded 59400 crops from ['/content/gdrive/My Drive/data/generated/SecondStage_X/training_rot3.record']\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n",
            "(59400, 12)\n",
            "12\n",
            "(59400, 360)\n",
            "360\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-19-19-14-r1/model-20.h5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}