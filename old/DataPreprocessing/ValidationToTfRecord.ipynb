{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* one set for eval during training and one set for final test\n",
    "* roughly class balanced\n",
    "* shuffled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as nr\n",
    "from random import shuffle\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from CNNRobotLocalisation.Utils.file_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE = \"Second\"\n",
    "if STAGE == \"First\":\n",
    "    PATH = '/media/data/LocalizationDataNew/Output/ValidationTFRecordDistr3'\n",
    "else:\n",
    "    # Compression artifacts of recompression in distr3 seem to disturb second stage\n",
    "    PATH = '/media/data/LocalizationDataNew/Output/Rev5'\n",
    "LABEL_MAP_PATH = '/media/data/Dokumente/Nextcloud/Studium/Bachelorarbeit/CNNRobotLocalisation/LabelMaps/robot_label_map.pbtxt'\n",
    "\n",
    "# Remarks on balancing:\n",
    "# Identifications of robots can't be balanced due to their number. \n",
    "# This has to be considered for the evaluation total metric.\n",
    "# It's not critical as the amount of images per identification only affects the resolution of the validation metric.\n",
    "# Factors to balance: lighting for robot type (ground color only for natural spheros)\n",
    "\n",
    "if STAGE == \"First\":\n",
    "    copter_led_classes = 1\n",
    "    copter_letter_classes = 1\n",
    "    sphero_classes = 1\n",
    "    youbot_classes = 1\n",
    "else:\n",
    "    copter_led_classes = 15\n",
    "    copter_letter_classes = 3\n",
    "    sphero_classes = 8\n",
    "    youbot_classes = 3\n",
    "    \n",
    "summarized_ds = {\n",
    "    # (balancing factor, dataset_list)\n",
    "    'by_al_copter_led': (0.25*copter_led_classes,['180420_by_al_eval_copter_led.avi','180420_by_al_eval_copter_led2.avi']),\n",
    "    'by_nl_copter_led': (0.25*copter_led_classes,['180420_by_nl_eval_copter_led.avi']),\n",
    "    'by_al_copter_letter': (0.25*copter_letter_classes,['180420_by_al_eval_copter_A.avi','180420_by_al_eval_copter_B.avi','180420_by_al_eval_copter_C.avi']),\n",
    "    'by_nl_copter_letter': (0.25*copter_letter_classes,['180420_by_nl_eval_copter_A.avi','180420_by_nl_eval_copter_B.avi','180420_by_nl_eval_copter_C.avi']),\n",
    "    # not labeled..\n",
    "    #'by_al_sphero': (0.5,['180413_by_al_sphero_rolling_1.avi','180413_by_al_sphero_rolling_2.avi']),\n",
    "    'by_nl_sphero': (0.25*sphero_classes,['180420_by_nl_sphero_eval.avi','180420_by_nl_sphero_eval2.avi']),\n",
    "    #'bg_al_sphero': (0.5*sphero_classes,['180417_bg_al_sphero_rolling.avi','180417_bg_al_sphero_rolling_distr.avi']),\n",
    "    'bg_al_sphero': (0.5*sphero_classes,['180417_bg_al_sphero_rolling.avi']), #without obstacles\n",
    "    #'bg_nl_sphero': (0.25*sphero_classes,['180417_bg_nl_sphero_rolling.avi','180417_bg_nl_sphero_rolling2.avi']),\n",
    "    'bg_nl_sphero': (0.25*sphero_classes,['180417_bg_nl_sphero_rolling.avi']), #without obstacles\n",
    "    'bg_al_youbot_mp1': (0.25*youbot_classes,['180420_by_al_youbot_A_eval.avi','180420_by_al_youbot_B_eval.avi','180420_by_al_youbot_C_eval.avi']),\n",
    "    'bg_al_youbot_mp2': (0.25*youbot_classes,['180417_bg_al_youbot_mp2_A_eval.avi','180417_bg_al_youbot_mp2_B_eval.avi','180417_bg_al_youbot_mp2_C_eval.avi']),\n",
    "    'bg_nl_youbot_mp2': (0.5*youbot_classes,['180417_bg_nl_youbot_mp2_A_eval.avi','180417_bg_nl_youbot_mp2_B_eval.avi','180417_bg_nl_youbot_mp2_C_eval.avi']),    \n",
    "}\n",
    "label_map_dict = label_map_util.get_label_map_dict(LABEL_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_example(f,dataset):\n",
    "    data = parseXML(f)['annotation']\n",
    "    with tf.gfile.GFile(f.replace('.xml','.jpg'), 'rb') as fid:\n",
    "        encoded_image_data = fid.read()\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmins, ymins, xmaxs, ymaxs = [],[],[],[]\n",
    "    subclasses_text,subclasses,classes_text,classes = [],[],[],[]\n",
    "    orientations = []\n",
    "    for obj in data['object']:\n",
    "        if 'undefined' in obj['name'] or ('pose_defined' in obj and obj['pose_defined']==str(0)):\n",
    "            return None\n",
    "        xmins.append(float(obj['bndbox']['xmin']) / width)\n",
    "        ymins.append(float(obj['bndbox']['ymin']) / height)\n",
    "        xmaxs.append(float(obj['bndbox']['xmax']) / width)\n",
    "        ymaxs.append(float(obj['bndbox']['ymax']) / height)\n",
    "        orientations.append(float(obj['pose']))\n",
    "        #TODO: dist_to_cam        \n",
    "        class_text = obj['name'].split('/')[0]\n",
    "        classes_text.append(class_text.encode('utf8'))\n",
    "        classes.append(label_map_dict[class_text])\n",
    "        subclass_text = obj['name'].replace('/','_')\n",
    "        subclasses_text.append(subclass_text.encode('utf8'))\n",
    "        subclasses.append(label_map_dict[subclass_text])\n",
    "        \n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/dataset': dataset_util.bytes_feature(dataset.encode('utf8')),\n",
    "        'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "        'image/format': dataset_util.bytes_feature(b'jpg'),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/pose/orientation': dataset_util.float_list_feature(orientations),\n",
    "        #'image/object/pose/iframe_w': dataset_util.float_list_feature(iframe_w),\n",
    "        #'image/object/pose/iframe_h': dataset_util.float_list_feature(iframe_h),\n",
    "        #'image/object/pose/dist_to_cam': dataset_util.float_list_feature(dist_to_cam),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/subclass/text': dataset_util.bytes_list_feature(subclasses_text),\n",
    "        'image/object/subclass/label': dataset_util.int64_list_feature(subclasses),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "#ex = xml_to_example('/media/data/LocalizationDataNew/Output/ValidationTFRecord/180420_copter/180420_by_al_eval_copter_led.avi/BoundingBoxes/frame000700.xml')\n",
    "#print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STAGE=='First':\n",
    "    num_per_type = 1400\n",
    "else:\n",
    "    num_per_type = 110\n",
    "\n",
    "datasets = summarized_ds\n",
    "print(datasets.keys())\n",
    "\n",
    "for i,k in enumerate(datasets.keys()):\n",
    "    num_balanced = round(num_per_type * datasets[k][0])\n",
    "    examples = []\n",
    "    fs = get_recursive_file_list(PATH,file_matchers = datasets[k][1], file_extensions=['.xml'])\n",
    "    assert len(fs) > 0\n",
    "    shuffle(fs)\n",
    "    for f in fs:\n",
    "        if len(examples) >= num_balanced:\n",
    "            break\n",
    "        ex = xml_to_example(f,k)\n",
    "        if ex != None:    \n",
    "            examples.append(ex)\n",
    "    assert (len(examples) == num_balanced), \\\n",
    "        'Not enough valid examples ({0} vs {1}) in dataset: {2}'.format(\n",
    "            len(examples),num_balanced,k)\n",
    "    shuffle(examples)\n",
    "    print('Created tf record for {} with {} samples.'.format(k,num_balanced))\n",
    "    writer = tf.python_io.TFRecordWriter(PATH+'/eval_{0}.record'.format(k,num_balanced))\n",
    "    for example in examples:\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old first stage approach\n",
    "\n",
    "record_categories = ['copter','sphero','youbot']\n",
    "num_per_cat = 200\n",
    "\n",
    "for cat in record_categories:\n",
    "    examples = []\n",
    "    datasets = []\n",
    "    for k in summarized_ds.keys():\n",
    "        if cat in k: datasets += summarized_ds[k][1]\n",
    "    print(datasets)\n",
    "    # balancing of datasets for each category\n",
    "    for i,d in enumerate(datasets):\n",
    "        fs = get_recursive_file_list(PATH,file_matchers = [d], file_extensions=['.xml'])\n",
    "        assert len(fs) > 0\n",
    "        shuffle(fs)\n",
    "        for f in fs:\n",
    "            if len(examples) >= round((i+1)*num_per_cat/len(datasets)):\n",
    "                break\n",
    "            ex = xml_to_example(f)\n",
    "            if ex != None:    \n",
    "                examples.append(ex)\n",
    "        assert (len(examples) == round((i+1)*num_per_cat/len(datasets))), \\\n",
    "            'Not enough valid examples ({0} vs {1}) in dataset: {2}'.format(\n",
    "                len(examples),round((i+1)*num_per_cat/len(datasets)),d)\n",
    "    shuffle(examples)\n",
    "    writer = tf.python_io.TFRecordWriter(PATH+'/eval-firststage-'+cat+\".record\")\n",
    "    for example in examples:\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea for second stage/final eval\n",
    "\n",
    "num_examples_per_category = 80\n",
    "summarized_ds = {\n",
    "    # (categories)\n",
    "    'by_al_copter_led': (15,['180420_by_al_eval_copter_led.avi','180420_by_al_eval_copter_led2.avi']),\n",
    "    'by_nl_copter_led': (15,['180420_by_nl_eval_copter_led.avi']),\n",
    "    #'by_al_copter_letter': (1,['180420_by_al_eval_copter_A.avi','180420_by_al_eval_copter_B.avi','180420_by_al_eval_copter_C.avi']),\n",
    "    #'by_nl_copter_letter': (1,['180420_by_nl_eval_copter_A.avi','180420_by_nl_eval_copter_B.avi','180420_by_nl_eval_copter_C.avi']),\n",
    "    'by_nl_sphero': (8,['180420_by_nl_sphero_eval.avi','180420_by_nl_sphero_eval2.avi']),\n",
    "    'bg_al_sphero': (8,['180417_bg_al_sphero_rolling.avi','180417_bg_al_sphero_rolling_distr.avi']),\n",
    "    'bg_nl_sphero': (8,['180417_bg_nl_sphero_rolling.avi','180417_bg_nl_sphero_rolling2.avi']),\n",
    "    'bg_al_youbot_mp1': (1,['180420_by_al_youbot_A_eval.avi','180420_by_al_youbot_B_eval.avi','180420_by_al_youbot_C_eval.avi']),\n",
    "    'bg_al_youbot_mp2': (1,['180417_bg_al_youbot_mp2_A_eval.avi','180417_bg_al_youbot_mp2_B_eval.avi','180417_bg_al_youbot_mp2_C_eval.avi']),\n",
    "    'bg_nl_youbot_mp2': (1,['180417_bg_nl_youbot_mp2_A_eval.avi','180417_bg_nl_youbot_mp2_B_eval.avi','180417_bg_nl_youbot_mp2_C_eval.avi']),    \n",
    "}\n",
    "for k in summarized_ds.keys():\n",
    "    print(k)\n",
    "    num_ds = len(summarized_ds[k])\n",
    "    num_ex = num_examples_per_category * summarized_ds[k][0]\n",
    "    print(num_ex)\n",
    "    files = get_recursive_file_list(PATH,file_matchers = summarized_ds[k][1])\n",
    "    print(len(files))\n",
    "    nr.choice(summarized_ds[k][1])\n",
    "    \n",
    "    # TODO: filter pose_defined, category_defined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
