{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Object Detector.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEEXkUq389y7",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow Object Detection Colab Framework\n",
        "\n",
        "This notebook provides an out-of-the-box training pipeline for an Object Detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHtT74fn9SIs",
        "colab_type": "text"
      },
      "source": [
        "## Mounting your Google Drive\n",
        "\n",
        "The easies way to make large files, such as your training images, available to Colab is by uploading them to your Google Drive and then linking the respective paths. Colab can access Google Drive after it has been mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyL0XGrVikGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg8B2pwM9Zez",
        "colab_type": "text"
      },
      "source": [
        "## Paths to configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTA918r9cu-",
        "colab_type": "text"
      },
      "source": [
        "The training directory, where the model checkpoints are going to be stored during and at the end of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "85edZ2kBDQbz",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/training/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2za29g4RNtB",
        "colab_type": "text"
      },
      "source": [
        "The output directory, where the model is going to be exported to as a frozen inference graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1177BfhRN3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_directory = \"/content/gdrive/My\\ Drive/RollingSwarm/fine_tuned_model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGoahn3f9skf",
        "colab_type": "text"
      },
      "source": [
        "The filenames for the TFRecords files, as well as the label_map.pbtxt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0EIICRbfxD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_record_fname = '/content/gdrive/My Drive/RollingSwarm/output/FirstStage_X/validation_width400.record'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZtAka8tf8-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_record_fname = '/content/gdrive/My Drive/RollingSwarm/output/FirstStage_X/training_width400.record'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUP4jV7OiKgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map_pbtxt_fname = '/content/gdrive/My Drive/RollingSwarm/label_map.pbtxt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caGpFw1R9zPy",
        "colab_type": "text"
      },
      "source": [
        "## Configure your main training parameters\n",
        "\n",
        "These include the number of training and evaluation steps, as well as the default model to be used (which will be automatically downloaded from the Tensorflow Research git repository)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8QUTNF6eLUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training steps.\n",
        "num_steps = 500\n",
        "\n",
        "# Number of evaluation steps.\n",
        "num_eval_steps = 500\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "        'batch_size': 8\n",
        "    }\n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "selected_model = 'ssd_mobilenet_v2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSJ4FI9O-GKg",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the framework\n",
        "\n",
        "The following cells clone the Tensorflow Models repository and install all necessary dependencies such as the Protobuf compiler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpDuREHXfCtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-T5dDsofPS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm6tn9lxfUFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd models/research"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P01HsgqPfgoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNmhuQRfic7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFVMAvxBfo-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVV1oLSAlBRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/models/research"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIiGMFmiEiis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# Optionally remove content in output model directory to fresh start.\n",
        "!rm -rf {model_dir}\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDYCg5kBlUZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-7Tb9WVlaTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n38N1nWld-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlOgyZa3lkVh",
        "colab_type": "text"
      },
      "source": [
        "## Configuring the Training Pipeline\n",
        "\n",
        "Here the default pipeline file for the selected model is loaded. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE5m7jnGlpqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfaNKmaMl205",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DorP_5E_RRTk",
        "colab_type": "text"
      },
      "source": [
        "## Replacing default pipeline config values\n",
        "\n",
        "Via regex, certain parameters in the pipeline file are amended to finetune the training process. The following "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTSPHO2fl8EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    \n",
        "    # ----------------------------------------------\n",
        "    # The following lines work well with the ssd_mobilenet_v2. When using any other model, the respective pipeline file will most likely have different parameters.\n",
        "    # Feel free to comment out some of the following expressions as you see fit.\n",
        "    # ----------------------------------------------\n",
        "\n",
        "    # Set minimum scale\n",
        "    s = re.sub('min_scale: [0-9].[0-9]+',\n",
        "               'min_scale: {}'.format(0.021875), s)\n",
        "    \n",
        "    # Set maximum scale\n",
        "    s = re.sub('max_scale: [0-9].[0-9]+',\n",
        "               'max_scale: {}'.format(0.02917), s)\n",
        "    \n",
        "    # Remove useless aspect ratios\n",
        "    s = re.sub('aspect_ratios: [0, 2-9].[0-9]+\\n',\n",
        "               '', s)\n",
        "    \n",
        "    # Set default training height\n",
        "    s = re.sub('height: [0-9]+',\n",
        "               'height: {}'.format(300), s)\n",
        "    \n",
        "    # Set default training height\n",
        "    s = re.sub('width: [0-9]+',\n",
        "               'width: {}'.format(400), s)\n",
        "    \n",
        "    # Set default kernel size\n",
        "    s = re.sub('kernel_size: [0-9]+',\n",
        "               'kernel_size: {}'.format(1), s)\n",
        "    \n",
        "    # Set dropout\n",
        "#    s = re.sub('use_dropout: false',\n",
        "#               'use_dropout: true', s)\n",
        "    \n",
        "    # Set dropout keep probability\n",
        "#    s = re.sub('dropout_keep_probability: [0-9].[0-9]+',\n",
        "#               'dropout_keep_probability: {}'.format(0.6), s)\n",
        "\n",
        "    # Amend the optimizer from RMS Prop to Adam\n",
        "    s = re.sub('rms_prop_optimizer: [^}]+}[^}]+}[^}]+}',\n",
        "               'adam_optimizer: {\\n      learning_rate: {\\n        constant_learning_rate {\\n          learning_rate: 0.001\\n        }\\n}\\n}', s)\n",
        "\n",
        "    # Change loss type\n",
        "    s = re.sub('CLASSIFICATION',\n",
        "               'BOTH', s)\n",
        "    \n",
        "    # Replace random cropping by vertical\n",
        "    s = re.sub('ssd_random_crop',\n",
        "               'random_vertical_flip', s)\n",
        "\n",
        "    # ----------------------------------------------\n",
        "    # End of custom parameters\n",
        "    # ----------------------------------------------\n",
        "\n",
        "    f.write(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W275k33ll-6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat {pipeline_fname}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFR1cVakmG-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBMyxL3cuepM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/training/ --port 6001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3n_TkPAujHk",
        "colab_type": "text"
      },
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jMKkpNju79_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "lst = os.listdir('/content/training/')\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L81QcXdu-CM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls {output_directory}"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}