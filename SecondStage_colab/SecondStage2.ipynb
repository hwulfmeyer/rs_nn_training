{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecondStage2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJlG_lwKEaQv",
        "colab_type": "text"
      },
      "source": [
        "to do:\n",
        "- image in tensorboard (test alpha mobilenet)\n",
        "- imports aufräumen\n",
        "- Secondstage.py & second_stage_utils.py importieren\n",
        "- ss_utils -> Bild öffnen\n",
        "- Konfiguration außerhalb:\n",
        "    - Name output\n",
        "    - cat/ bin/ reg\n",
        "- endliche Anzahl an Runden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHIKxNY3oahf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VO6zc_v7lJ-",
        "colab_type": "text"
      },
      "source": [
        "von Tobi geklaut\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_HjEoWRonVL",
        "colab_type": "code",
        "outputId": "0a32f4fb-cf08-498b-b773-e477c30f2304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH4BMdyyorVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85f2szA_pAJg",
        "colab_type": "code",
        "outputId": "c907909c-2dbc-4494-bd85-83943fd9daaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd models/research"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPwg_kkapFJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsskp5qWpJRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJLXPuYIpNg4",
        "colab_type": "code",
        "outputId": "5a67c6dc-5760-4b8b-d791-5fac98d59672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTest.test_create_experimental_model\n",
            "[       OK ] ModelBuilderTest.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 17 tests in 0.170s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "japGTz-bFnCC",
        "colab_type": "code",
        "outputId": "b51564ed-ad91-47a9-da2d-b122fbf51c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5h9RbyaYC-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/SecondStage_colab/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZWtkiVKShNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['PYTHONPATH'] += ':/content/gdrive/My Drive/SecondStage_colab/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exoPdhiJ7p1R",
        "colab_type": "text"
      },
      "source": [
        "Second Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwRKHys0KBHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NAME =\n",
        "#example: cat_exptype_dataset(training)_dataset(eval)_conf(batch size etc) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqzYclgV64T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_PATH = '/content/gdrive/My Drive/SecondStage_colab/output/'\n",
        "LABEL_MAP_PATH = '/content/gdrive/My Drive/SecondStage_colab/label_map.pbtxt'\n",
        "TRAIN_RECORD = '/content/gdrive/My Drive/data/train/training_rot6.record'\n",
        "EVAL_RECORD = '/content/gdrive/My Drive/data/train/evaluation_rot2.record'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjrsBWe_7sjQ",
        "colab_type": "code",
        "outputId": "d292b4fe-b115-4516-9238-976dbb9186f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/')\n",
        "\n",
        "from SecondStage_colab import second_stage_utils\n",
        "from SecondStage_colab import file_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKaABMtXB-OA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/models/research')\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R7pUpyVcHIf",
        "colab_type": "text"
      },
      "source": [
        "configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1i_AkgfcGUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = True\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "ANGLES_PER_BIN = 1\n",
        "NUM_ORI_BINS = 360"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Hbp0ghcZtB",
        "colab_type": "text"
      },
      "source": [
        "label map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk0ZiaTCthBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/SecondStage_colab')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOwjdrteceEe",
        "colab_type": "code",
        "outputId": "5f15e2e7-0bf4-4f00-f848-21a3f224e19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile label_map.pbtxt\n",
        "\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'bright_blue'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'bright_red'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'bright_green'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 4\n",
        "  name: 'bright_white'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 5\n",
        "  name: 'dark_blue'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 6\n",
        "  name: 'dark_green'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 7\n",
        "  name: 'dark_red'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5-N8oYNQ_8C",
        "colab_type": "text"
      },
      "source": [
        "experiment definitioin\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmm4SaY3QXt3",
        "colab_type": "code",
        "outputId": "8d67e86d-f9ae-4726-8929-1ced5f3d7a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile exp_def.py\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "default_sstage_conf = {\n",
        "    'dataset': 'default',\n",
        "    'types': ['copter','sphero','youbot'],\n",
        "    # converged after 15 / 10 / 5 epochs\n",
        "    # 10 epochs after convergance for evaluation phase\n",
        "    'epochs_cat': 120,\n",
        "    #'epochs_reg': 20,\n",
        "    'epochs_reg': 120,\n",
        "    'epochs_bin': 120,\n",
        "    'optimizer': 'adam',\n",
        "    'learning_rate': 3e-4,\n",
        "    'dropout': 0,\n",
        "    'alpha': 0.5,\n",
        "    'img_size': 35,\n",
        "    'separate_cat_ori': True,\n",
        "    'cat_weight': 1.0,\n",
        "    'reg_weight': 1.0,\n",
        "    'bin_weight': 1.0,\n",
        "    'enable_reg': False,\n",
        "    'enable_bin': True,\n",
        "    'repetions': 4,\n",
        "}\n",
        "\n",
        "def create_all_sstage_experiments():\n",
        "    configs = []\n",
        "    configs.extend(create_sstage_default())\n",
        "\n",
        "    return configs\n",
        "\n",
        "def create_sstage_default():\n",
        "    config = []\n",
        "    modified = deepcopy(default_sstage_conf)\n",
        "    modified['name'] = \"sstage_default\"\n",
        "    modified['enable_reg'] = True\n",
        "    config.append(deepcopy(modified))\n",
        "    return config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting exp_def.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDJatt09QfGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "from datetime import datetime\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from keras import applications\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Reshape, Conv2D, Activation\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, Callback\n",
        "from keras.models import load_model\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "import csv, json, pickle\n",
        "from lxml import etree\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJTQhRbOVOFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_square(im, size, fill_color=(0, 0, 0, 0)):\n",
        "    x, y = im.size\n",
        "    scl = size/max(x, y)\n",
        "    im = im.resize((int(x*scl),int(y*scl)))\n",
        "    new_im = Image.new('RGB', (size, size), fill_color)\n",
        "    new_im.paste(im, (int((size - im.size[0]) / 2), int((size - im.size[1]) / 2)))\n",
        "    return new_im\n",
        "\n",
        "def angle_diff2(y_true, y_pred):\n",
        "    return tf.mod(( (y_true - y_pred) + 180 ), 360 ) - 180\n",
        "\n",
        "def np_angle_diff2(y_true, y_pred):\n",
        "    return np.mod(( (y_true - y_pred) + 180 ), 360 ) - 180\n",
        "\n",
        "def angle_mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(angle_diff2(y_true, y_pred)), axis=-1)\n",
        "\n",
        "def angle_mse(y_true, y_pred):\n",
        "    return K.mean(K.square(angle_diff2(y_true, y_pred)), axis=-1)\n",
        "\n",
        "def angle_bin_error(y_true, y_pred):\n",
        "    diff = angle_diff2(K.argmax(y_true)*ANGLES_PER_BIN,\n",
        "                       K.argmax(y_pred)*ANGLES_PER_BIN)\n",
        "    return K.mean(K.cast(K.abs(diff), K.floatx()))\n",
        "\n",
        "def data_to_keras(X,Y,Z, num_classes, size=35):\n",
        "    X = [np.asarray(e) for e in X]\n",
        "\n",
        "    X = np.asarray(X, dtype=\"uint8\")\n",
        "    X = X.reshape(X.shape[0], size, size, 3)\n",
        "    X.astype('float32')\n",
        "    #X /= 255\n",
        "    Y = np.asarray(Y)\n",
        "    Y = np_utils.to_categorical(Y, num_classes)\n",
        "    Z = np.asarray(Z)\n",
        "    return X,Y,Z\n",
        "\n",
        "def angle_to_bin(Z):\n",
        "    Z = np.mod(Z, 360)\n",
        "    return np_utils.to_categorical(np.floor(Z/ANGLES_PER_BIN),NUM_ORI_BINS)\n",
        "\n",
        "\"\"\"\n",
        "img_enc = (feats.feature['image/encoded'].bytes_list.value[0]).decode('utf-8')\n",
        "img_name = (feats.feature['image/filename'].bytes_list.value[0]).decode('utf-8')\n",
        "filename = (feats.feature['image/filename'].bytes_list.value[0]).decode('utf-8')\n",
        "source_id = (feats.feature['image/source_id'].bytes_list.value[0]).decode('utf-8')\n",
        "width = feats.feature['image/width'].int64_list.value[0]\n",
        "height = feats.feature['image/height'].int64_list.value[0]\n",
        "class = (feats.feature['image/object/class/text'].bytes_list.value[0]).decode('utf-8')\n",
        "class_label = feats.feature['image/object/class/label'].int64_list.value[0]\n",
        "color = (feats.feature['image/object/subclass/text'].bytes_list.value[0]).decode('utf-8')\n",
        "color_id = feats.feature['image/object/subclass/label'].int64_list.value[0]\n",
        "orientation = feats.feature['image/object/pose/orientation'].int64_list.value[0]\n",
        "\"\"\"\n",
        "\n",
        "def tf_record_load_crops(files,num_per_record=-1,size=35):\n",
        "    crops, classes, orientations = [],[],[]\n",
        "    debug_infos = []\n",
        "    for f in files:\n",
        "        record_iterator = tf.python_io.tf_record_iterator(f)\n",
        "        #records = tf.data.TFRecordDataset(f)\n",
        "        for l, string_record in enumerate(record_iterator):\n",
        "        #for string_record in records:\n",
        "            if num_per_record!=-1 and l > num_per_record: break\n",
        "            example = tf.train.Example()\n",
        "            example.ParseFromString(string_record)#.numpy())\n",
        "            feats = example.features\n",
        "\n",
        "            img_enc = (feats.feature['image/encoded'].bytes_list.value[0])#.decode('utf-8')\n",
        "\n",
        "            img_name = (feats.feature['image/filename'].bytes_list.value[0]).decode('utf-8')\n",
        "            img_enc = (feats.feature['image/encoded'].bytes_list.value[0]) #.decode('utf-8')\n",
        "            width = feats.feature['image/width'].int64_list.value[0]\n",
        "            height = feats.feature['image/height'].int64_list.value[0]\n",
        "\n",
        "            #img = Image.frombytes('RGB', (height, width), img_enc)\n",
        "            img = Image.open(io.BytesIO(img_enc))\n",
        "\n",
        "            color = (feats.feature[\"image/object/subclass/text\"].bytes_list.value[0]).decode('utf-8') \n",
        "            color_id = feats.feature[\"image/object/subclass/label\"].int64_list.value[0]\n",
        "            orientation = feats.feature[\"image/object/pose/orientation\"].int64_list.value[0]\n",
        "\n",
        "            crops.append(make_square(img,size))\n",
        "            classes.append(color_id)\n",
        "            orientations.append(orientation)\n",
        "            debug_infos.append({\n",
        "                'filename': img_name,\n",
        "                'src_record': f\n",
        "            })\n",
        "\n",
        "    assert len(crops) == len(classes) and len(crops) == len(orientations)\n",
        "    print(\"Loaded {} crops from {}\".format(len(crops),files))\n",
        "\n",
        "    return crops, classes, orientations, debug_infos\n",
        "\n",
        "def custom_randint(min, max):\n",
        "    min = round(min)\n",
        "    max = round(max)\n",
        "    if min == max:\n",
        "        return min\n",
        "    return randint(min, max)\n",
        "\n",
        "def tf_record_extract_crops(files, num_derivations,\n",
        "                            out_var, in_var,\n",
        "                            num_per_record=-1,\n",
        "                            size=35,\n",
        "                            class_filters=None):\n",
        "    crops, classes, orientations = [],[],[]\n",
        "    debug_infos = []\n",
        "    for f in files:\n",
        "        record_iterator = tf.python_io.tf_record_iterator(f)\n",
        "        for l, string_record in enumerate(record_iterator):\n",
        "            if num_per_record!=-1 and l > num_per_record: break\n",
        "            example = tf.train.Example()\n",
        "            example.ParseFromString(string_record) #.numpy())\n",
        "            feats = example.features\n",
        "            width  = feats.feature[\"image/width\"].int64_list.value[0]\n",
        "            height = feats.feature[\"image/height\"].int64_list.value[0]\n",
        "            img_name = (feats.feature['image/filename'].bytes_list.value[0]).decode('utf8')\n",
        "            img_enc = (feats.feature['image/encoded'].bytes_list.value[0])\n",
        "            img = Image.open(io.BytesIO(img_enc))\n",
        "            #img = Image.open(open(img_enc, \"rb\"))\n",
        "            #img = Image.frombytes('RGB', (height, width), img_enc)\n",
        "\n",
        "            for i,_ in enumerate(feats.feature[\"image/object/class/text\"].bytes_list.value):\n",
        "                class_text = (feats.feature[\"image/object/subclass/text\"].bytes_list.value[i]).decode('utf8')\n",
        "                if not (class_filters == None or any(m in class_text for m in class_filters)):\n",
        "                    continue\n",
        "                class_label = feats.feature[\"image/object/subclass/label\"].int64_list.value[i]\n",
        "                orientation = feats.feature[\"image/object/pose/orientation\"].int64_list.value[i]\n",
        "                width = feats.feature['image/width'].int64_list.value[0]\n",
        "                height = feats.feature['image/height'].int64_list.value[0]                \n",
        " \n",
        "                img_resized = img;\n",
        "                if (width != 35 or height != 35): \n",
        "                    img_resized = img.resize((35,35), Image.BICUBIC)\n",
        "\n",
        "                crops.append(make_square(img_resized,size))\n",
        "                classes.append(class_label)\n",
        "                orientations.append(orientation)\n",
        "                debug_infos.append({\n",
        "                    'filename': img_name,\n",
        "                    'src_record': f,\n",
        "                    'crop_num': i*num_derivations,\n",
        "                })\n",
        "            img.close()\n",
        "\n",
        "    return crops, classes, orientations, debug_infos\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "import sklearn.metrics as sklm\n",
        "class TensorBoardCustom(Callback):\n",
        "    def __init__(self, log_dir='./logs',label_map=''):\n",
        "        super(Callback, self).__init__()\n",
        "        global tf, projector\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            from tensorflow.contrib.tensorboard.plugins import projector\n",
        "        except ImportError:\n",
        "            raise ImportError('You need the TensorFlow module installed to use TensorBoard.')\n",
        "\n",
        "        self.log_dir = log_dir\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "        if K.backend() == 'tensorflow':\n",
        "            self.sess = K.get_session()\n",
        "        self.merged = tf.summary.merge_all()\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
        "\n",
        "    def write_metric(self, name, value, epoch, namespace=''):\n",
        "        if namespace != '':\n",
        "            namespace += '/'\n",
        "        summary = tf.Summary(value=[\n",
        "            tf.Summary.Value(tag=namespace+name, simple_value=value),\n",
        "        ])\n",
        "        self.writer.add_summary(summary, epoch)\n",
        "\n",
        "    def add_custom_metrics(self, epoch):\n",
        "        y_pred, z_pred, z2_pred = self.model.predict(self.validation_data[0])\n",
        "        # [1] -> Y; [2] -> Z\n",
        "        y_targ = self.validation_data[1]\n",
        "        y_targ_onehot = np.argmax(y_targ, axis=1)\n",
        "        y_pred_onehot = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        class_prec, class_rec, class_f1, class_support = sklm.precision_recall_fscore_support(\n",
        "            y_targ_onehot,\n",
        "            y_pred_onehot,\n",
        "            labels=[int(e) for e in self.label_map.keys()]\n",
        "        )\n",
        "        for i, (prec, rec, f1, sup) in enumerate(zip(class_prec,\n",
        "                                                   class_rec,\n",
        "                                                   class_f1,\n",
        "                                                   class_support)):\n",
        "            i+=1\n",
        "            if self.label_map == '':\n",
        "                label = str(i)\n",
        "            elif i in self.label_map:\n",
        "                label = str(i) +' '+ self.label_map[i]['name']\n",
        "            else:\n",
        "                label = str(i)\n",
        "            if sup > 0:\n",
        "                namespace = \"Precision by Category/{}\".format(label)\n",
        "                self.write_metric('Precision', prec, epoch, namespace)\n",
        "                namespace = \"Recall by Category/{}\".format(label)\n",
        "                self.write_metric('Recall', rec, epoch, namespace)\n",
        "                namespace = \"F1 by Category/{}\".format(label)\n",
        "                self.write_metric('F1', f1, epoch, namespace)\n",
        "            if epoch == 0:\n",
        "                namespace = \"Support by Category/{}\".format(label)\n",
        "                self.write_metric('Support', sup, epoch, namespace)\n",
        "        # average='weighted'\n",
        "        # Calculate metrics for each label, and find their average\n",
        "        # -> balanced\n",
        "        avg_prec, avg_rec, avg_f1, avg_support = sklm.precision_recall_fscore_support(\n",
        "            y_targ_onehot,\n",
        "            y_pred_onehot,\n",
        "            average='weighted'\n",
        "        )\n",
        "        self.write_metric('Average Precision', avg_prec, epoch, 'Average Performance')\n",
        "        self.write_metric('Average Recall', avg_rec, epoch, 'Average Performance')\n",
        "        self.write_metric('Average F1', avg_f1, epoch, 'Average Performance')\n",
        "        self.write_metric('Average Support', avg_support, epoch, 'Average Performance')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        for name, value in logs.items():\n",
        "            if name in ['batch', 'size']:\n",
        "                continue\n",
        "            # summary = tf.Summary()\n",
        "            # summary_value = summary.value.add()\n",
        "            # summary_value.simple_value = value.item()\n",
        "            # summary_value.tag = name\n",
        "            if 'val' in name:\n",
        "                group = 'Keras Validation/'\n",
        "            else:\n",
        "                group = 'Keras Training/'\n",
        "            self.write_metric(name, value, epoch, group)\n",
        "\n",
        "        # self.add_custom_metrics(epoch)\n",
        "\n",
        "        self.writer.flush()\n",
        "\n",
        "    def on_train_end(self, _):\n",
        "        self.writer.close()\n",
        "\n",
        "\n",
        "import unittest\n",
        "\n",
        "class TestSecondStageUtils(unittest.TestCase):\n",
        "\n",
        "    def test_np_angle_diff2(self):\n",
        "        self.assertEqual(np_angle_diff2(359,52), -53)\n",
        "        self.assertEqual(np_angle_diff2(52,359), 53)\n",
        "        self.assertEqual(np_angle_diff2(0,720), 0)\n",
        "\n",
        "    def test_angle_to_bin(self):\n",
        "        self.assertEqual(len(angle_to_bin(4)), 90)\n",
        "        self.assertEqual(angle_to_bin(4)[0], 0)\n",
        "        self.assertEqual(angle_to_bin(4)[1], 1)\n",
        "        for i in range(2,90):\n",
        "            self.assertEqual(angle_to_bin(4)[i], 0)\n",
        "        self.assertEqual(angle_to_bin(50)[12], 1)\n",
        "        self.assertEqual(angle_to_bin(360)[0], 1)\n",
        "        self.assertEqual(angle_to_bin(370)[2], 1)\n",
        "        self.assertEqual(angle_to_bin(-10)[87], 1)\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    unittest.main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG9YYdriX8WZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_record, conf, types, out, rep=1):\n",
        "\n",
        "    #from SecondStage_colab.file_utils import *\n",
        "    from file_utils import save_json\n",
        "\n",
        "    timestamp = \"{:%Y-%m-%d-%H-%M}\".format(datetime.now())\n",
        "    log_path = LOG_PATH + conf['name'] + '_' + \\\n",
        "                ''.join(types) + out + '/' + \\\n",
        "                timestamp + '-r' + str(rep) + '/'\n",
        "    os.makedirs(log_path, exist_ok=True)\n",
        "    save_json(log_path + '/experiment_config.json', conf)\n",
        "\n",
        "    label_map = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
        "    #label_map = create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
        "\n",
        "    num_classes = label_map_util.get_max_label_map_index(\n",
        "                           label_map_util.load_labelmap(LABEL_MAP_PATH)) + 1\n",
        "    print(\"x\")\n",
        "    X,Y,Z,_ = tf_record_load_crops([train_record])\n",
        "    \n",
        "    X_train, Y_train, Z_train = data_to_keras(X,Y,Z,num_classes,conf['img_size'])\n",
        "    Z2_train = angle_to_bin(Z_train)\n",
        "    print(\"´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\")\n",
        "    print(Y_train.shape)\n",
        "    print(num_classes)\n",
        "    print(Z2_train.shape)\n",
        "    print(NUM_ORI_BINS)\n",
        "    print(\"´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\")\n",
        "\n",
        "    eval_records = EVAL_RECORD\n",
        "\n",
        "    X,Y,Z,_ = tf_record_extract_crops([eval_records], 1, 0.0, 0.0)\n",
        "    X_val, Y_val, Z_val = data_to_keras(X,Y,Z,num_classes,conf['img_size'])\n",
        "    Z2_val = angle_to_bin(Z_val)\n",
        "\n",
        "    assert len(X_val) > 0 and len(Y_val) > 0 and len(Z_val) > 0, \\\n",
        "        '{} is incomplete'.format(eval_records)\n",
        "    mobilenet_base = applications.mobilenet.MobileNet(alpha = conf['alpha'],\n",
        "                                                      weights = \"imagenet\",\n",
        "                                                      include_top=False,\n",
        "                                                      input_shape = (\n",
        "                                                      conf['img_size'],\n",
        "                                                      conf['img_size'],\n",
        "                                                        3\n",
        "                                                      ))\n",
        "    shape = (1, 1, int(1024 * conf['alpha']))\n",
        "    x = GlobalAveragePooling2D()(mobilenet_base.output)\n",
        "    x = Reshape(shape, name='reshape_1')(x)\n",
        "    x = Dropout(conf['dropout'], name='dropout')(x)\n",
        "    # Branch regression\n",
        "    reg = Conv2D(1, (1, 1),\n",
        "               padding='same', name='conv_reg')(x)\n",
        "    reg = Activation('linear', name='act_linear')(reg)\n",
        "    reg = Reshape((1,), name='reg_out')(reg)\n",
        "    # Branch orientation classification with bins\n",
        "    bin = Conv2D(NUM_ORI_BINS, (1, 1),\n",
        "               padding='same', name='conv_bin')(x)\n",
        "    bin = Activation('softmax', name='act_bin')(bin)\n",
        "    bin = Reshape((NUM_ORI_BINS,), name='bin_out')(bin)\n",
        "    # Branch classification\n",
        "    cat = Conv2D(num_classes, (1, 1),\n",
        "               padding='same', name='conv_cat')(x)\n",
        "    cat = Activation('softmax', name='act_softmax')(cat)\n",
        "    cat = Reshape((num_classes,), name='cat_out')(cat)\n",
        "\n",
        "    # creating the final model\n",
        "    model_final = None\n",
        "    model_final = Model(inputs = mobilenet_base.input, outputs = [cat,reg,bin])\n",
        "\n",
        "    if conf['optimizer'] == 'rmsprop':\n",
        "        optimizer = optimizers.RMSprop(lr=conf['learning_rate'])\n",
        "    elif conf['optimizer'] == 'adam':\n",
        "        optimizer = optimizers.Adam(lr=conf['learning_rate'])\n",
        "    else:\n",
        "        raise Error('Unknown optimizer: ' + conf['optimizer'])\n",
        "\n",
        "    model_final.compile(optimizer = optimizer,\n",
        "                        loss={'cat_out': 'categorical_crossentropy',\n",
        "                              'reg_out': angle_mse,\n",
        "                              'bin_out': 'categorical_crossentropy',\n",
        "                        },\n",
        "                        loss_weights={'cat_out': conf['cat_weight'],\n",
        "                                      'reg_out': conf['reg_weight'],\n",
        "                                      'bin_out': conf['bin_weight']},\n",
        "                        metrics ={'cat_out': 'accuracy',\n",
        "                                  'reg_out': angle_mae,\n",
        "                                  'bin_out': angle_bin_error})\n",
        "\n",
        "    summary = TensorBoardCustom(log_dir=log_path,label_map=label_map)\n",
        "    filepath=log_path+\"model-{epoch:02d}.h5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, verbose=1, period=5)\n",
        "\n",
        "    \"\"\"callbacks=[summary,checkpoint],\"\"\"\n",
        "\n",
        "    model_final.fit(\n",
        "        X_train,\n",
        "        {'cat_out': Y_train, 'reg_out': Z_train, 'bin_out': Z2_train},\n",
        "        validation_data=(X_val,[Y_val,Z_val, Z2_val]),\n",
        "        batch_size=BATCH_SIZE, epochs=conf['epochs'], verbose=0,\n",
        "        callbacks=[summary,checkpoint],\n",
        "        shuffle=True\n",
        "    )\n",
        "    model_final.save(log_path+\"model-final.h5\")\n",
        "    print(\"Finished training for {}_{}\".format(conf['name'],t))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjhzN9oBxzHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from SecondStage_colab import exp_def\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jru6gJiHAVl",
        "colab_type": "code",
        "outputId": "5beb5782-8926-4d1b-83c9-c7d95c27dc7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "exp = exp_def.create_all_sstage_experiments()\n",
        "for or_conf in tqdm(exp):\n",
        "    print(\"or_conf\", or_conf)\n",
        "    for r in range(or_conf['repetions']): \n",
        "        #train_records = get_recursive_file_list(TRAIN_DIR,file_matchers=[or_conf['dataset']])\n",
        "        train_records = [TRAIN_RECORD]\n",
        "        for t in or_conf['types']:\n",
        "            for out in ['_cat','_reg','_bin','']:\n",
        "                conf = deepcopy(or_conf)\n",
        "                train_instance_name = conf['name']+'_'+t+out\n",
        "                if not conf['separate_cat_ori'] and out != '':\n",
        "                    continue\n",
        "                if conf['separate_cat_ori']:\n",
        "                    if out == '':\n",
        "                        continue\n",
        "                    elif out == '_cat':\n",
        "                        conf['reg_weight'] = 0.0\n",
        "                        conf['bin_weight'] = 0.0\n",
        "                        conf['epochs'] = conf['epochs_cat']\n",
        "                    elif out == '_reg' and conf['enable_reg']:\n",
        "                        conf['cat_weight'] = 0.0\n",
        "                        conf['bin_weight'] = 0.0\n",
        "                        conf['epochs'] = conf['epochs_reg']\n",
        "                    elif out == '_bin' and conf['enable_bin']:\n",
        "                        conf['cat_weight'] = 0.0\n",
        "                        conf['reg_weight'] = 0.0\n",
        "                        conf['epochs'] = conf['epochs_bin']\n",
        "                    else:\n",
        "                        print('UNKNOWN OUTPUT CONFIG {}'.format(out))\n",
        "                        continue\n",
        "                if not re.match('sstage_default_sphero_cat', train_instance_name): \n",
        "                    print('Skip '+train_instance_name)\n",
        "                    continue            \n",
        "                #record_for_type = [e for e in train_records if t in e]\n",
        "                #print(\"record_for_type: \", len(record_for_type))\n",
        "                #assert len(record_for_type) == 1, \\\n",
        "                #       \"{} for {} has not one item\".format(record_for_type, train_instance_name)\n",
        "                print(\"Start training\")\n",
        "                \n",
        "                if not GPU:\n",
        "                    p = Process(\n",
        "                      target=train,\n",
        "                      args=(train_records[0], conf, ['sphero'], out, r)\n",
        "                    )\n",
        "                    p.start() \n",
        "                else:\n",
        "                 \n",
        "                  train(train_records[0], conf, ['sphero'], out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "or_conf {'dataset': 'default', 'types': ['copter', 'sphero', 'youbot'], 'epochs_cat': 120, 'epochs_reg': 120, 'epochs_bin': 120, 'optimizer': 'adam', 'learning_rate': 0.0003, 'dropout': 0, 'alpha': 0.5, 'img_size': 35, 'separate_cat_ori': True, 'cat_weight': 1.0, 'reg_weight': 1.0, 'bin_weight': 1.0, 'enable_reg': True, 'enable_bin': True, 'repetions': 4, 'name': 'sstage_default'}\n",
            "Skip sstage_default_copter_cat\n",
            "Skip sstage_default_copter_reg\n",
            "Skip sstage_default_copter_bin\n",
            "Start training\n",
            "x\n",
            "Loaded 64800 crops from ['/content/gdrive/My Drive/data/train/training_rot6.record']\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n",
            "(64800, 8)\n",
            "8\n",
            "(64800, 360)\n",
            "360\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00005: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-05.h5\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-10.h5\n",
            "\n",
            "Epoch 00015: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-15.h5\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-20.h5\n",
            "\n",
            "Epoch 00025: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-25.h5\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-30.h5\n",
            "\n",
            "Epoch 00035: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-35.h5\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-40.h5\n",
            "\n",
            "Epoch 00045: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-45.h5\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-50.h5\n",
            "\n",
            "Epoch 00055: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-55.h5\n",
            "\n",
            "Epoch 00060: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-60.h5\n",
            "\n",
            "Epoch 00065: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-65.h5\n",
            "\n",
            "Epoch 00070: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-70.h5\n",
            "\n",
            "Epoch 00075: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-75.h5\n",
            "\n",
            "Epoch 00080: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-80.h5\n",
            "\n",
            "Epoch 00085: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-85.h5\n",
            "\n",
            "Epoch 00090: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-90.h5\n",
            "\n",
            "Epoch 00095: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-95.h5\n",
            "\n",
            "Epoch 00100: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-100.h5\n",
            "\n",
            "Epoch 00105: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-105.h5\n",
            "\n",
            "Epoch 00110: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-110.h5\n",
            "\n",
            "Epoch 00115: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-115.h5\n",
            "\n",
            "Epoch 00120: saving model to /content/gdrive/My Drive/SecondStage_colab/output/sstage_default_sphero_cat/2020-01-15-19-59-r1/model-120.h5\n",
            "Finished training for sstage_default_sphero\n",
            "Skip sstage_default_sphero_reg\n",
            "Skip sstage_default_sphero_bin\n",
            "Skip sstage_default_youbot_cat\n",
            "Skip sstage_default_youbot_reg\n",
            "Skip sstage_default_youbot_bin\n",
            "Skip sstage_default_copter_cat\n",
            "Skip sstage_default_copter_reg\n",
            "Skip sstage_default_copter_bin\n",
            "Start training\n",
            "x\n",
            "Loaded 64800 crops from ['/content/gdrive/My Drive/data/train/training_rot6.record']\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n",
            "(64800, 8)\n",
            "8\n",
            "(64800, 360)\n",
            "360\n",
            "´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´´\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}